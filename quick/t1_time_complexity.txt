Θ(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such 
                 that 0 <= c1*g(n) <= f(n) <= c2*g(n) for all n >= n0}

O(g(n)) = {f(n): there exist positive constants c and 
                 n0 such that 0 <= f(n) <= c*g(n) for 
                 all n >= n0}

Ω(g(n)) = {f(n): there exist positive constants c and
                 n0 such that 0 <= c*g(n) <= f(n) for
                 all n >= n0}

-> Note: Big-Ο is used as a tight upper-bound on the growth of an algorithm’s effort (this effort is described by the function f(n)), even though, as written, it can also be a loose upper-bound. “Little-ο” (ο()) notation is used to describe an upper-bound that cannot be tight. Same goes for "Big-Omega" and "Little-Omega".

`Little-Oh`
    f(n) = ο(g(n)) if for any real constant c > 0, there exists an integer constant n0 ≥ 1 such that 0 ≤ f(n) < c*g(n) for every integer n ≥ n0
    In mathematical relation, f(n) = o(g(n)) means lim f(n)/g(n) = 0 as n→∞

`Little-Omega`
    f(n) = ω(g(n)) if for any real constant c > 0, there exists an integer constant n0 ≥ 1 such that f(n) > c*g(n) ≥ 0 for every integer n ≥ n0
    In mathematical relation, f(n) = ω(g(n)) means lim f(n)/g(n) = ∞ as n→∞

-> Note: f(n) ∈ ω(g(n)) if and only if g(n) ∈ ο((f(n))

`Articles`
- https://www.geeksforgeeks.org/analysis-of-algorithms-set-4-analysis-of-loops/
- https://www.geeksforgeeks.org/time-complexity-loop-loop-variable-expands-shrinks-exponentially/

`Problems`
- https://www.interviewbit.com/problems/reccmpl1/
- https://www.interviewbit.com/problems/reccmpl2/
- https://www.interviewbit.com/problems/reccmpl3/
- https://www.interviewbit.com/problems/amortized1/
- https://www.interviewbit.com/problems/choose2/
- https://www.interviewbit.com/problems/nestedcmpl3/
